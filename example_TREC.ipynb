{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pickle\n",
    "from saliency.core.base import *\n",
    "from saliency.core import IntegratedGradients\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import ig2\n",
    "from ig2 import REP_LAYER_VALUES, REP_DISTANCE_GRADIENTS\n",
    "from models.model import TREC_CNN\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TREC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_TREC():\n",
    "    data = {}\n",
    "\n",
    "    def read(mode):\n",
    "        x, y = [], []\n",
    "\n",
    "        with open(\"data/TREC/TREC_\" + mode + \".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line[-1] == \"\\n\":\n",
    "                    line = line[:-1]\n",
    "                y.append(line.split()[0].split(\":\")[0])\n",
    "                x.append(line.split()[1:])\n",
    "\n",
    "        # x, y = shuffle(x, y)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            dev_idx = len(x) // 10\n",
    "            data[\"dev_x\"], data[\"dev_y\"] = x[:dev_idx], y[:dev_idx]\n",
    "            data[\"train_x\"], data[\"train_y\"] = x[dev_idx:], y[dev_idx:]\n",
    "        else:\n",
    "            data[\"test_x\"], data[\"test_y\"] = x, y\n",
    "\n",
    "    read(\"train\")\n",
    "    read(\"test\")\n",
    "\n",
    "    return data\n",
    "\n",
    "data = read_TREC()\n",
    "data[\"vocab\"] = sorted(list(set([w for sent in data[\"train_x\"] + data[\"dev_x\"] + data[\"test_x\"] for w in sent])))\n",
    "data[\"classes\"] = sorted(list(set(data[\"train_y\"])))\n",
    "data[\"word_to_idx\"] = {w: i for i, w in enumerate(data[\"vocab\"])}\n",
    "data[\"idx_to_word\"] = {i: w for i, w in enumerate(data[\"vocab\"])}\n",
    "params = {\n",
    "    \"MODEL\":\"pretrained\",\n",
    "    \"MAX_SENT_LEN\": max([len(sent) for sent in data[\"train_x\"] + data[\"dev_x\"] + data[\"test_x\"]]),\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"WORD_DIM\": 300,\n",
    "    \"VOCAB_SIZE\": len(data[\"vocab\"]),\n",
    "    \"CLASS_SIZE\": len(data[\"classes\"]),\n",
    "    \"FILTERS\": [3, 4, 5],\n",
    "    \"FILTER_NUM\": [100, 100, 100],\n",
    "    \"DROPOUT_PROB\": 0.5,\n",
    "    \"NORM_LIMIT\": 3,\n",
    "    \"GPU\":'cuda:7'\n",
    "}\n",
    "cls_fullname_dict ={'ABBR':'ABBREVIATION','DESC':'DESCRIPTION','ENTY':'ENTITY','HUM':'HUMAN',\n",
    "                        'LOC':'LOCATION','NUM':'NUMERIC'}\n",
    "\n",
    "class_idx_str = 'class_idx_str'\n",
    "sentence_x_word, sentence_y_name = data[\"train_x\"][:500], data[\"train_y\"][:500]\n",
    "sentence_x = [[data[\"word_to_idx\"][w] if w in data[\"vocab\"] else params[\"VOCAB_SIZE\"] for w in sent] +\n",
    "        [params[\"VOCAB_SIZE\"] + 1] * (params[\"MAX_SENT_LEN\"] - len(sent))\n",
    "        for sent in sentence_x_word]\n",
    "sentence_y = np.array([data[\"classes\"].index(c) for c in sentence_y_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load question classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TREC_CNN(**params).to(params[\"GPU\"])\n",
    "model.load_state_dict( torch.load(\"models/TREC_CNN.pkl\"))\n",
    "model.eval()\n",
    "rep_layer = model.fc\n",
    "rep_layer_outputs = {}\n",
    "def rep_layer_forward(m, i, o):\n",
    "    rep_layer_outputs[REP_LAYER_VALUES] = i[0]\n",
    "forward_hook = rep_layer.register_forward_hook(rep_layer_forward)\n",
    "emb_min, emb_max = model.embedding.weight.min().item(), model.embedding.weight.max().item()\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    return model.embedding(sentence)\n",
    "\n",
    "def get_prediction(embedding):\n",
    "    WORD_DIM = model.WORD_DIM\n",
    "    MAX_SENT_LEN = model.MAX_SENT_LEN\n",
    "    x = embedding.view(-1, 1, WORD_DIM * MAX_SENT_LEN)\n",
    "    conv_results = [\n",
    "        F.max_pool1d(F.relu(model.get_conv(i)(x)), MAX_SENT_LEN - model.FILTERS[i] + 1)\n",
    "            .view(-1, model.FILTER_NUM[i])\n",
    "        for i in range(len(model.FILTERS))]\n",
    "\n",
    "    x = torch.cat(conv_results, 1)\n",
    "    x = F.dropout(x, p=model.DROPOUT_PROB, training=model.training)\n",
    "    x = model.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx_str = 'class_idx_str'\n",
    "def call_model_function(explicand_emb, call_model_args=None, expected_keys=None):\n",
    "    explicand_emb = torch.tensor(explicand_emb, dtype=torch.float32).to(params[\"GPU\"])\n",
    "    explicand_emb.requires_grad = True\n",
    "    logits = get_prediction(explicand_emb)\n",
    "    target_class_idx =  torch.LongTensor(np.array(call_model_args[class_idx_str]).reshape(-1,1)).to(params[\"GPU\"])\n",
    "    # logits = model(sentence)\n",
    "    # m = torch.nn.Softmax(dim=1)\n",
    "    # output = m(logits)\n",
    "    if INPUT_OUTPUT_GRADIENTS in expected_keys:\n",
    "        # outputs = logits[:,target_class_idx]\n",
    "        if logits.size(0)>1 and target_class_idx.size(0) == 1:\n",
    "            target_class_idx = target_class_idx.repeat(logits.size(0),1) \n",
    "        target_class_idx = torch.zeros_like(logits).scatter_(1, target_class_idx, 1).detach()        \n",
    "        grads = torch.autograd.grad(logits, explicand_emb, grad_outputs=target_class_idx)\n",
    "        gradients = grads[0].cpu().detach().numpy()\n",
    "        # To word-level gradient (sum across embedding dimension)\n",
    "        # gradients = np.sum(gradients,axis=-1)\n",
    "        return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
    "\n",
    "    if REP_LAYER_VALUES in expected_keys:        \n",
    "        return rep_layer_outputs\n",
    "\n",
    "    if REP_DISTANCE_GRADIENTS in expected_keys:\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        baseline_conv = call_model_args['layer_baseline']\n",
    "        input_conv = rep_layer_outputs[REP_LAYER_VALUES]\n",
    "        loss = -1 * loss_fn(input_conv, baseline_conv)\n",
    "        loss.backward()\n",
    "        grads = explicand_emb.grad.data\n",
    "        gradients = grads.cpu().detach().numpy()\n",
    "        return {REP_DISTANCE_GRADIENTS: gradients,\n",
    "                'loss':loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IG2 attribution for ten samples\n",
    "\n",
    "The attribution are calulated on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'What', 'country', 'was', 'A', 'Terrible', 'Beauty', 'to', 'Leon', 'Uris', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.42010876536369324\n",
      "100 iterations, rep distance Loss -0.15807844698429108\n",
      "200 iterations, rep distance Loss -0.04529042914509773\n",
      "300 iterations, rep distance Loss -0.019004816189408302\n",
      "400 iterations, rep distance Loss -0.008226913399994373\n",
      "500 iterations, rep distance Loss -0.002988226944580674\n",
      "Integrate gradients on GradPath...\n",
      "['DESC', 'What', 'is', 'typhoid', 'fever', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.3230636417865753\n",
      "100 iterations, rep distance Loss -0.12282904982566833\n",
      "200 iterations, rep distance Loss -0.04098369553685188\n",
      "300 iterations, rep distance Loss -0.01772630587220192\n",
      "400 iterations, rep distance Loss -0.007620504125952721\n",
      "500 iterations, rep distance Loss -0.002859528176486492\n",
      "Integrate gradients on GradPath...\n",
      "['LOC', 'Where', 'can', 'I', 'buy', 'a', 'hat', 'like', 'the', 'kind', 'Jay', 'Kay', 'from', 'Jamiroquai', 'wears', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.48200613260269165\n",
      "100 iterations, rep distance Loss -0.2333114892244339\n",
      "200 iterations, rep distance Loss -0.0969325453042984\n",
      "300 iterations, rep distance Loss -0.03898054361343384\n",
      "400 iterations, rep distance Loss -0.01752522401511669\n",
      "500 iterations, rep distance Loss -0.008190792053937912\n",
      "Integrate gradients on GradPath...\n",
      "['DESC', 'What', 'are', 'the', 'Twin', 'Cities', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.2825893461704254\n",
      "100 iterations, rep distance Loss -0.12962916493415833\n",
      "200 iterations, rep distance Loss -0.05067252367734909\n",
      "300 iterations, rep distance Loss -0.02247614972293377\n",
      "400 iterations, rep distance Loss -0.010641064494848251\n",
      "500 iterations, rep distance Loss -0.004566354677081108\n",
      "Integrate gradients on GradPath...\n",
      "['ENTY', 'What', 'is', 'Beethoven', \"'s\", '9th', 'symphony', 'called', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.2810240685939789\n",
      "100 iterations, rep distance Loss -0.12110619246959686\n",
      "200 iterations, rep distance Loss -0.04685475677251816\n",
      "300 iterations, rep distance Loss -0.022373877465724945\n",
      "400 iterations, rep distance Loss -0.010929025709629059\n",
      "500 iterations, rep distance Loss -0.00499725341796875\n",
      "Integrate gradients on GradPath...\n",
      "['HUM', 'Name', 'the', 'lawyer', 'for', 'Randy', 'Craft', '.']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.3424602746963501\n",
      "100 iterations, rep distance Loss -0.16908203065395355\n",
      "200 iterations, rep distance Loss -0.06780814379453659\n",
      "300 iterations, rep distance Loss -0.02779800072312355\n",
      "400 iterations, rep distance Loss -0.013321480713784695\n",
      "500 iterations, rep distance Loss -0.006067612674087286\n",
      "Integrate gradients on GradPath...\n",
      "['ENTY', 'Which', 'breakfast', 'cereal', 'brought', 'you', '``', 'the', 'best', 'each', 'morning', \"''\", '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.320121169090271\n",
      "100 iterations, rep distance Loss -0.1452796757221222\n",
      "200 iterations, rep distance Loss -0.05815902724862099\n",
      "300 iterations, rep distance Loss -0.025240348652005196\n",
      "400 iterations, rep distance Loss -0.011658484116196632\n",
      "500 iterations, rep distance Loss -0.004923035390675068\n",
      "Integrate gradients on GradPath...\n",
      "['ABBR', 'What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.3364306688308716\n",
      "100 iterations, rep distance Loss -0.16506727039813995\n",
      "200 iterations, rep distance Loss -0.06809797883033752\n",
      "300 iterations, rep distance Loss -0.028563745319843292\n",
      "400 iterations, rep distance Loss -0.012579233385622501\n",
      "500 iterations, rep distance Loss -0.0051332274451851845\n",
      "Integrate gradients on GradPath...\n",
      "['LOC', 'What', 'is', 'the', 'Homelite', 'Inc.', 'home', 'page', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.2842418849468231\n",
      "100 iterations, rep distance Loss -0.12261027097702026\n",
      "200 iterations, rep distance Loss -0.05064653977751732\n",
      "300 iterations, rep distance Loss -0.023917799815535545\n",
      "400 iterations, rep distance Loss -0.011443731375038624\n",
      "500 iterations, rep distance Loss -0.005135038401931524\n",
      "Integrate gradients on GradPath...\n",
      "['ENTY', 'What', 'explosive', 'do', 'you', 'get', 'by', 'mixing', 'charcoal', ',', 'sulfur', 'and', 'saltpeter', '?']\n",
      "GradPath search...\n",
      "0 iterations, rep distance Loss -0.2791922688484192\n",
      "100 iterations, rep distance Loss -0.10411455482244492\n",
      "200 iterations, rep distance Loss -0.03767216578125954\n",
      "300 iterations, rep distance Loss -0.017528140917420387\n",
      "400 iterations, rep distance Loss -0.007680858019739389\n",
      "500 iterations, rep distance Loss -0.0029200995340943336\n",
      "Integrate gradients on GradPath...\n"
     ]
    }
   ],
   "source": [
    " # Only uses the samples with different labels to explained y as the references\n",
    "def select_counter_refs(explained_y):\n",
    "    ref_idx = []\n",
    "    for i in range(100,120): # Randomly choose from 20 samples\n",
    "        if sentence_y[i]!=explained_y:\n",
    "            ref_idx.append(i)\n",
    "    return  np.array(sentence_x)[ref_idx,:]\n",
    "\n",
    "ig2_mask_list = []\n",
    "exp_idx_list = np.arange(100,110) # Randomly choose 10 sentences\n",
    "for exp_idx in exp_idx_list:\n",
    "    print([sentence_y_name[exp_idx]]+sentence_x_word[exp_idx])\n",
    "    explicand = np.array(sentence_x)[exp_idx,:]\n",
    "    explicand_emb = get_embedding(torch.LongTensor(explicand).to(params[\"GPU\"])).detach().cpu().numpy()\n",
    "    prediction = torch.argmax(get_prediction(torch.tensor(explicand_emb).to(params[\"GPU\"])),dim=1).cpu().numpy()\n",
    "    assert prediction==sentence_y[exp_idx]\n",
    "\n",
    "    ref_sentence = select_counter_refs(prediction)\n",
    "    reference_emb = get_embedding(torch.LongTensor(ref_sentence).to(params[\"GPU\"])).detach().cpu().numpy()\n",
    "    \n",
    "    call_model_args = {class_idx_str: prediction}\n",
    "    explainer = ig2.IG2()\n",
    "    ig2_mask = explainer.GetMask(explicand_emb,reference_emb,\n",
    "            call_model_function,call_model_args,steps=501,step_size=0.01,clip_min_max=[emb_min,emb_max],)\n",
    "    ig2_mask = np.sum(ig2_mask,axis=-1)\n",
    "    ig2_mask_list.append(ig2_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla IG attribution with zero baseline\n",
    "The basic benchmarkto be compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'What', 'country', 'was', 'A', 'Terrible', 'Beauty', 'to', 'Leon', 'Uris', '?']\n",
      "['DESC', 'What', 'is', 'typhoid', 'fever', '?']\n",
      "['LOC', 'Where', 'can', 'I', 'buy', 'a', 'hat', 'like', 'the', 'kind', 'Jay', 'Kay', 'from', 'Jamiroquai', 'wears', '?']\n",
      "['DESC', 'What', 'are', 'the', 'Twin', 'Cities', '?']\n",
      "['ENTY', 'What', 'is', 'Beethoven', \"'s\", '9th', 'symphony', 'called', '?']\n",
      "['HUM', 'Name', 'the', 'lawyer', 'for', 'Randy', 'Craft', '.']\n",
      "['ENTY', 'Which', 'breakfast', 'cereal', 'brought', 'you', '``', 'the', 'best', 'each', 'morning', \"''\", '?']\n",
      "['ABBR', 'What', 'does', 'Ms.', ',', 'Miss', ',', 'and', 'Mrs.', 'stand', 'for', '?']\n",
      "['LOC', 'What', 'is', 'the', 'Homelite', 'Inc.', 'home', 'page', '?']\n",
      "['ENTY', 'What', 'explosive', 'do', 'you', 'get', 'by', 'mixing', 'charcoal', ',', 'sulfur', 'and', 'saltpeter', '?']\n"
     ]
    }
   ],
   "source": [
    "ig_mask_list = []\n",
    "for exp_idx in exp_idx_list:\n",
    "    print([sentence_y_name[exp_idx]]+sentence_x_word[exp_idx])\n",
    "    explicand = np.array(sentence_x)[exp_idx,:]\n",
    "    explicand_emb = get_embedding(torch.LongTensor(explicand).to(params[\"GPU\"])).detach().cpu().numpy()\n",
    "    prediction = torch.argmax(get_prediction(torch.tensor(explicand_emb).to(params[\"GPU\"])),dim=1).cpu().numpy()\n",
    "    assert prediction==sentence_y[exp_idx]\n",
    "    \n",
    "    call_model_args = {class_idx_str: prediction}\n",
    "    black = np.zeros_like(explicand_emb)\n",
    "    ig_mask_r = IntegratedGradients().GetMask(\n",
    "            explicand_emb, call_model_function, call_model_args, x_steps=200, x_baseline=black, batch_size=32)\n",
    "    ig_mask_r = np.sum(ig_mask_r,axis=-1)\n",
    "    ig_mask_list.append(ig_mask_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word attributions in sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took the code for this cell block from \n",
    "#  https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html\n",
    "from matplotlib.colors import Normalize, rgb2hex\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.backends.backend_pgf import FigureCanvasPgf\n",
    "mpl.use('ps')\n",
    "mpl.backend_bases.register_backend('pdf', FigureCanvasPgf)\n",
    "from matplotlib import rc\n",
    "rc('text',usetex=True)\n",
    "rc('pgf', preamble=r'\\usepackage{xcolor}') \n",
    "rc('text.latex', preamble=r'\\usepackage{xcolor}') #!not pgf\n",
    "\n",
    "def hlstr(string, color='white'):\n",
    "    if string == '&':\n",
    "        string = '\\&'\n",
    "    return r\"{\\setlength{\\fboxsep}{3pt}\\colorbox[HTML]{\"+f\"{color}\"+\"}{\"+f\"{string}\"+\"}}\"\n",
    "\n",
    "def colorize(attrs, cmap='PiYG'):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    # cmap = sns.diverging_palette(232, 10, s=82, l=40, n=9, sep=10, center=\"dark\", as_cmap=True)\n",
    "    # attrs = attrs / np.sum(np.abs(attrs[:len(sent)]))\n",
    "    cmap_bound = np.max(np.abs(attrs))\n",
    "    attrs = attrs.tolist()\n",
    "    norm = Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "\n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: rgb2hex(cmap(norm(x))).split('#')[-1].upper(), attrs))\n",
    "    return colors\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis([0, 9, 0, 9])\n",
    "plt.axis('off')\n",
    "plt.text(0.3,10,\"[Predictions]\",\n",
    "    va='top',ha='left',fontsize=6.5,wrap=True)\n",
    "plt.text(2.0,10,\"[Questions with explanations]\",\n",
    "    va='top',ha='left',fontsize=6.5,wrap=True)\n",
    "for n, exp_idx in enumerate(exp_idx_list):\n",
    "    base = 9.5 - 1.1*n\n",
    "    sent = sentence_x_word[exp_idx]\n",
    "    plt.text(0.0,base-0.35,f\"\\#{n+1}\",\n",
    "        va='top',ha='left',fontsize=5,wrap=True)\n",
    "\n",
    "    plt.text(0.3,base-0.35,f\"[{cls_fullname_dict[sentence_y_name[exp_idx]]}]\",\n",
    "        va='top',ha='left',fontsize=5,wrap=True)\n",
    "\n",
    "    plt.text(1.35,base,r\"{\\setlength{\\fboxsep}{3pt}\\colorbox{white}{[IG\\textsuperscript{2}]}}\",\n",
    "    va='top',ha='left',fontsize=6.3,wrap=True)\n",
    "    plt.text(1.8,base,\"\".join(list(map(hlstr, sent, colorize(ig2_mask_list[n])))),\n",
    "        va='top',ha='left',fontsize=6.3,wrap=True)\n",
    "    plt.text(1.35,base-0.45,r\"{\\setlength{\\fboxsep}{3pt}\\colorbox{white}{[IG]}}\",\n",
    "        va='top',ha='left',fontsize=6.3,wrap=True)\n",
    "    plt.text(1.8,base-0.45,\"\".join(list(map(hlstr, sent, colorize(ig_mask_list[n])))),\n",
    "        va='top',ha='left',fontsize=6.3,wrap=True)\n",
    "        \n",
    "plt.tight_layout()    \n",
    "plt.savefig('results/TREC_IG2toIG.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f08a82dfb69131bd1f745de8f8d464f8f6601c2c9b52d54236c2e838d5deea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
